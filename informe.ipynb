{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 1 - Aprendiz de damas chinas\n",
    "\n",
    "### Grupo 13:\n",
    "     - J. Aguirre  C.I: 4.773.509-6\n",
    "     - A. Collazo C.I: 4.455.617-4\n",
    "     - G. Núnez C.I: 4.785.081-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal objetivo de esta tarea es construir un jugador que aprenda a jugar a las damas chinas siguendo los lineamientos expuestos en el capítulo 1 del libro del curso.\n",
    "    \n",
    "El éxito del aprendizaje se medirá a través de la cantidad de partidas ganadas sobre un total de partidas jugadas previamente configurado.\n",
    "    \n",
    "El conjunto de entrenamiento será el resultado de las partidas jugadas ante dos tipos de adversarios: un jugador aleatorio, y un jugador que conforma una versión anterior del propio aprendiz.\n",
    "   \n",
    "Para valorar el aprendizaje se definirá una función objetivo cuyo resultado será un valor numérico que será más alto para aquellos tableros que sean más promisorios. Dicha función estará implementada a partir de una representación del tablero convenientemente elegida, a los efectos de aprender a jugar y conseguir mayor porcentaje de victorias a través de la experiencia.\n",
    "\n",
    "En el presente informe se describirá la solución implementada, los resultados intermedios obtenidos y se explicará el por qué de los mismos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente sección se mencionan las decisiones de diseño utilizadas en los distintos aspectos del juego, la representación lógica del tablero, los algoritmos utilizados para la implementación del aprendiz y la construcción de los jugadores oponentes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation*}\n",
       "    f(<t_0,t_1,...,t_{10}>) = \\begin{cases}\n",
       "               1              & t_1 = 10\\\\\n",
       "               -1               & t_2 = 10\\\\\n",
       "               \\sum t_i * w_i & \\text{sino}\n",
       "           \\end{cases}\n",
       "\\end{equation*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{equation*}\n",
    "    f(<t_0,t_1,...,t_{10}>) = \\begin{cases}\n",
    "               1              & t_1 = 10\\\\\n",
    "               -1               & t_2 = 10\\\\\n",
    "               \\sum t_i * w_i & \\text{sino}\n",
    "           \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2.1 Juego\n",
    "### 2.1.1 Reglas\n",
    "Las reglas utilizadas para el juego son las mismas que se aplican en el juego de las damas chinas convencional a excepción de las siguientes simplificaciones:\n",
    "* Dado que las partidas son siempre de dos jugadores el tablero fue simplificado, omitiendo cuatro de las seis puntas de la estrella. Por lo tanto el tablero resultante está formado por un hexágono de nueve casilleros de largo, y dos triángulos opuestos, de largo cuatro casilleros cada uno.\n",
    "* Una vez que una ficha alcanza su triángulo objetivo, la misma no podrá realizar movimientos hacia atrás en el tablero. Esta decisión fue tomada para que el jugador randómico no saque fichas de las posiciones ganadoras.\n",
    "* Se limita a poder realizar un único salto por jugada, ya que no se consideró como una casuística a suceder a menudo.\n",
    "* Se define un número máximo de jugadas realizadas por ambos jugadores durante todo el partido. En caso de que el jugador aprendiz no sea capaz de vencer a su oponente en menos de diez mil jugadas, el partido será computado como perdido para el jugador aprendiz si éste tiene menos de 5 fichas en posición ganadoras. De lo contrario será computado como ganador para el jugador aprendiz.\n",
    "* En el caso de atascamiento (EXPLICAR UN PCO MAS ESOT), se desechara la partida del conjunto de aprendizaje.\n",
    "\n",
    "## 2.2 Tablero\n",
    "Con el objetivo de contabilizar el grado de avance de las fichas (de cada jugador), el tablero fue particionado en cinco regiones lógicas: las fichas en posición inicial, en posición lejana del triángulo objetivo, las que están a mitad de camino, las cercanas al triángulo objetivo y finalmente las fichas en posición ganadora. Todas las regiones son de largo tres casilleros a excepción de la primer y última región que tienen largo cuatro cada una.\n",
    "A partir de esto, se define la representación del tablero como una tupla de diez valores, donde cada valor representa la cantidad de fichas de cada jugador en: (región inicial, región lejana, región media, región cercana, región ganadora).\n",
    "\n",
    "En las figuras 2.1 y 2.2 se muestran dos ejemplos de posibles tableros con sus respectivas tuplas que los representan. (PONER IMAGENES DE TABLEROS CON FICHAS EN DISTINTAS POSICIONES Y LA TUPLA CORRESPONDIENTE A ESA SITUACION) \n",
    "\n",
    "\n",
    "\n",
    "## 2.3 Algoritmo\n",
    "La función de valoración es una función partida en tres rangos:\n",
    "\\begin{equation*}\n",
    "    f(<t_0,t_1,...,t_{10}>) = \\begin{cases}\n",
    "               1              & t_1 = 10\\\\\n",
    "               -1               & t_2 = 10\\\\\n",
    "               \\sum t_i * w_i & \\text{sino}\n",
    "           \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Cada una de las variables está multiplicada por una ponderación que representa la relevancia del valor del atributo en la estrategia de juego.\n",
    "El dominio de la función de valoración está definida en todos los reales entre -1 y 1.\n",
    "\n",
    "Los función se ajusta siguiendo el algoritmo LMS al término de una cantidad de partidas dada. Dicha cantidad de partidos es configurable al inicio del entrenamiento. El vector de pesos obtenido es normalizdo de manera tal que el valor de cada uno de ellos se mantiene en el rango entre -1 y 1.\n",
    "En la sección 3.1 se tabulan los resultados obtenidos ajustando la función de valoración cada 1, 10, 100 y 1.000 partidos jugados.\n",
    "\n",
    "El ajuste de los pesos está afectado por una factor de aprendizaje ($\\mu$).\n",
    "Dicho factor de aprendizaje se ajusta según el resultado del partido, de la forma que sigue: \\begin{equation*}\n",
    "    f(win) = \\begin{cases}\n",
    "               0.0000001              & win = 1\\\\\n",
    "               0.0001               & \\text{sino}\n",
    "             \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "En caso de obtener una partida ganadora, el ajuste del aprendizaje se enfriará, ya que está cumpliendo el objetivo.\n",
    "\n",
    "Tal vez precisen agregar algo de pseudocódigo o código para ejemplificar:\n",
    "\n",
    "```python\n",
    "    def f(x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "## 2.4 Contrarios\n",
    "Se definen dos tipos de jugadores oponentes:\n",
    "* Un jugador que elige sus movimientos al azar\n",
    "* Un jugador que constituye una versión previa del jugador aprendiz actual.\n",
    "\n",
    "### 2.4.1 Jugador azaroso\n",
    "Las jugadas se eligen de forma aleatoria sobre un subconjunto de jugadas válidas para cierta ficha (elegida también de forma aleatoria). Como se mencionó anteriormente, una vez que una ficha alcanza la región ganadora se limita a que únicamente avance (en caso de ser posible) para llenar el triángulo del adversario e impedir que retire las fichas de dicha región.\n",
    "En caso de que no haya movimiento disponible para la ficha elegida, se selecciona otra elegida nuevamente al azar (notar que eventualmente podría llegar a ser la misma).\n",
    "\n",
    "### 2.4.2 Versión previa del aprendiz actual\n",
    "En primera instancia este jugador utiliza los pesos iniciales que tenía nuestro aprendiz ante su adversario azaroso, y el jugador aprendiz utilizará los pesos que obtuvo al terminar la última partida de entrenamiento ante el oponente aleatorio. Luego el jugador aprendiz actualiza los pesos conforme al parámetro ingresado que marca la cantidad de partidos que deben transcurrir para que este vuelva a ajustar los pesos. Una vez que el jugador aprendiz actualiza los nuevos pesos, la versión previa configurará como sus pesos actuales, los pesos que tenía el aprendiz hasta antes de actualizarlos por los nuevos.\n",
    "(NO SE SI ESTO ES REALMENTE ASI, LO PUSE PARA REDACTAR ALGO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presentan los resultados obtenidos durante el proceso de aprendizaje.\n",
    "- Presentar los distintos experimentos que se realizan y los resultados que se obtienen.\n",
    "\n",
    "- La información de los resultados se presenta en tablas y en gráficos, de acuerdo a su naturaleza. Por ejemplo:\n",
    "\n",
    "_En la gráfica 1, se observa el error cuadrático total del conjunto de entrenamiento a medida que pasan los juegos para el oponente X_\n",
    "\n",
    "### 3.1 Fase de entrenamiento vs. el jugador azaroso\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Total de partidos jugados</th>\n",
    "    <th>Frecuencia de ajuste</th>\n",
    "    <th>Partidos ganados</th>\n",
    "    <th>Partidos perdidos</th>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1.000</td>\n",
    "    <td>10</td>\n",
    "    <td>364</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>1.000</td>\n",
    "    <td>50</td>\n",
    "    <td>405</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1.000</td>\n",
    "    <td>100</td>\n",
    "    <td>403</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>1.000</td>\n",
    "    <td>250</td>\n",
    "    <td>413</td>\n",
    "    <td>0</td>\n",
    "  </tr>   \n",
    "    <caption>Tabla 1.1 - Entrenamiento del aprendiz ante el jugador aleatorio partiendo siempe desde los mismos pesos iniciales, variando la frecuencia de ajuste para una cantidad total de partidos fija.</caption>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Total de partidos jugados</th>\n",
    "    <th>Frecuencia de ajuste</th>\n",
    "    <th>Partidos ganados</th>\n",
    "    <th>Partidos perdidos</th>\n",
    "    <th>Porcentaje de partidos ganados</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>1</td>\n",
    "    <td>4</td>\n",
    "    <td>0</td>\n",
    "    <td>4%</td>  \n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>10</td>\n",
    "    <td>39</td>\n",
    "    <td>0</td>\n",
    "    <td>39%</td>  \n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>1.000</td>\n",
    "    <td>100</td>\n",
    "    <td>403</td>\n",
    "    <td>0</td>\n",
    "    <td>40.3%</td>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10.000</td>\n",
    "    <td>1000</td>\n",
    "    <td>3.862</td>\n",
    "    <td>0</td>\n",
    "    <td>38.62%</td>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100.000</td>\n",
    "    <td>10.000</td>\n",
    "    <td>38.616</td>\n",
    "    <td>0</td>\n",
    "    <td>38.61%</td>\n",
    "  </tr>   \n",
    "    <caption>Tabla 1.2 - Entrenamiento del aprendiz ante el jugador aleatorio partiendo siempe desde los mismos pesos iniciales.</caption>\n",
    "</table>\n",
    "\n",
    "\n",
    "### 3.2 Fase de entrenamiento vs. la versión previa del aprendiz actual\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Total de partidos jugados</th>\n",
    "    <th>Frecuencia de ajuste</th>\n",
    "    <th>Partidos ganados</th>\n",
    "    <th>Partidos perdidos</th>  \n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>47</td>\n",
    "    <td>27</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>2</td>\n",
    "    <td>32</td>\n",
    "    <td>34</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>10</td>\n",
    "    <td>37</td>\n",
    "    <td>25</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>25</td>\n",
    "    <td>28</td>\n",
    "    <td>37</td>\n",
    "  </tr>   \n",
    "    <caption>Tabla 2.1 - Entrenamiento del aprendiz ante la versión previa partiendo desde los pesos iniciales obtenidos en el mejor resultado del entrenamiento anterior, variando la frecuencia de ajuste para una cantidad total de partidos fija.</caption>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Total de partidos jugados</th>\n",
    "    <th>Frecuencia de ajuste</th>\n",
    "    <th>Partidos ganados</th>\n",
    "    <th>Partidos perdidos</th>\n",
    "    <th>Porcentaje de partidos ganados</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>1</td>\n",
    "    <td>4</td>\n",
    "    <td>3</td>\n",
    "    <td>40%</td>  \n",
    "  </tr>    \n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>100</td>\n",
    "    <td>10</td>\n",
    "    <td>37</td>\n",
    "    <td>25</td>\n",
    "    <td>37%</td>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1.000</td>\n",
    "    <td>100</td>\n",
    "    <td>35</td>\n",
    "    <td>32</td>\n",
    "    <td>-</td>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10.000</td>\n",
    "    <td>1000</td>\n",
    "    <td>-</td>\n",
    "    <td>-</td>\n",
    "    <td>-</td>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100.000</td>\n",
    "    <td>10.000</td>\n",
    "    <td>-</td>\n",
    "    <td>-</td>\n",
    "    <td>-</td>\n",
    "  </tr>   \n",
    "    <caption>Tabla 2.2 - Entrenamiento del aprendiz ante la versión partiendo desde los pesos iniciales obtenidos en el mejor resultado del entrenamiento anterior.</caption>\n",
    "</table>\n",
    "\n",
    "### 3.3 Competencia entre los jugadores obtenidos durante las fases de entrenamiento\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Total de partidos jugados</th>\n",
    "    <th>Partidos ganados 1</th>\n",
    "    <th>Partidos ganados 2</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>34</td>\n",
    "    <td>38</td> \n",
    "  </tr>    \n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>100</td>\n",
    "    <td>33</td>\n",
    "    <td>38</td> \n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>100</td>\n",
    "    <td>35</td>\n",
    "    <td>31</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>33</td>\n",
    "    <td>33</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>33</td>\n",
    "    <td>32</td> \n",
    "  </tr>  \n",
    "    <caption>Tabla 3.1 - Resultados de la competencia entre los jugadores obtenidos con cada entrenamiento .</caption>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b40cfa990>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.plot(range(0,100), [2**-(x/10) for x in range(0,100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Debe existir alguna instancia donde se compile la información relevante de los experimentos de forma de poder comparar fácilmente los distintos experimentos. Por ejemplo:\n",
    "\n",
    "_En la tabla 1, se presentan los distintos resultados contra el jugador aleatorio, para los distintos valores de $\\alpha$ elegidos. El mejor resultado se obtiene para $\\alpha=0.05$, lo que prueba que la estrategia..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una breve conclusión del trabajo realizado. Por ejemplo: \n",
    "\n",
    "A partir de los resultados obtenidos en el punto anterior se deducen varias conclusiones.\n",
    "Durante la fase de entrenamiento frente al jugador aleatorio, los resultados en general son buenos, dado que en todos los casos de prueba el jugador logra el cometido de incrementar la cantidad de partidos ganados a raíz de su experiencia.\n",
    "\n",
    "Los resultados obtenidos frente a la verión previa los resultados no son los esperados. Aquí se ve que el aprendizaje se frena bastante. Esto se debe a varias limitaciones que tiene el modelo propuesto:\n",
    "* Dividir el tablero en franjas de distancias no fue del todo bueno, ya que se ve que hay situaciones en donde el jugador valora de igual manera ir hacia atrás que ir hacia adelante (no se logra valorar el avance deseado).\n",
    "* La tupla elegida para representar el tablero contiene información relativa al jugador contrario, pero no se toman acciones que limiten su juego como el bloquearlo. Una posible mejora sería computar n jugadas siguientes de la instancia actual, de manera de evaluar un mayor conjunto de posibilidades (propias y del rival) asumiendo el costo computacional que eventualmente podría tener.\n",
    "* El aprendizaje nace desde el entrenamiento frente a un jugador aleatorio que rompe con la hipótesis de que en todo momento realizará el mejor movimiento posible. Este hecho puede llevar a condicionar la forma de jugar del jugador aprendiz. \n",
    "* \n",
    "\n",
    "Posibles mejoras del modelo:\n",
    "A la luz de los resultados obtenidos, podemos ver que el modelo utilizado no fue del todo bueno. Algunas mejoras que este podría tener serían:\n",
    "* Cambiar las regiones de distancia por la distancia propiamente dicha.\n",
    "* Considerar la posibilidad de bloquear las fichas del oponente.\n",
    "* Favorecer la estrategia de ataque utilizando saltos de largo mayor que 1.\n",
    "\n",
    "\n",
    "\n",
    "- ¿cuándo se dieron los mejores resultados del jugador?\n",
    "- ¿encuentra alguna relación con los parámetros / oponentes/ atributos elegidos?\n",
    "- ¿cómo mejoraría los resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
